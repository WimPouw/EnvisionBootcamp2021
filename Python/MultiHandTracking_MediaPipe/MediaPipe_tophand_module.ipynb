{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8028c32e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h1> MultiParty and Single Person Tracking with MediaPipe: Top- and Front-View Hand Tracking  </h1>\n",
    "\n",
    "\n",
    "<h3> Wim Pouw ( wim.pouw@donders.ru.nl )<br>James Trujillo ( james.trujillo@donders.ru.nl )<br>\n",
    "    18-11-2021 </h3>\n",
    "    \n",
    "<img src=\"./images/envision_banner.png\"> </center>\n",
    "\n",
    "<h3> Info documents </h3>\n",
    "In this module, we'll demonstrates how to perform motion tracking using the lightweight tool MediaPipe, and considers some of the pros and cons of this method. Specifically, we'll be using MediaPipe for hand-tracking in situations where a) we have multiple people in frame from a top view, b) a single person from top view, and c) a single person for front-view. Single person tracking is a more easily processable as we will explain, as it much easier to identify which hands belong to which person from frame to frame. In the case of multi-person hand tracking we require a little bit more post-processing to identify persons from frame to frame (the envision toolbox also contains a script for linking person from frame to frame in such cases).\n",
    "<br><br>\n",
    "\n",
    "* location code: \n",
    "https://github.com/WimPouw/EnvisionBootcamp2021/tree/main/Python/MediaBodyTracking\n",
    "\n",
    "* citation: \n",
    "Pouw, W.,  &  Trujillo, J.P. (2021-11-18). <i> MultiParty Tracking with MediaPipe: Top-View Hand Tracking  </i> \\[day you visited the site]. Retrieved from: https://github.com/WimPouw/EnvisionBootcamp2021/tree/main/Python/MediaBodyTracking \n",
    "\n",
    "\n",
    "<h3> Introduction </h3>\n",
    "Here we will cover how to utilize MediaPipe to acquire motion tracking of the hands, from multiple people. MediaPipe offers a nice lightweight (computationally) solution to capture hand motion from multiple people (or just one person). We'll first go over some code to get body and hand tracking. \n",
    "\n",
    "<h4>resources</h4>\n",
    "* https://github.com/google/mediapipe\n",
    "<br><br>\n",
    "* https://google.github.io/mediapipe/solutions/hands.html\n",
    "<br><br>\n",
    "* Lugaresi, C., Tang, J., Nash, H., McClanahan, C., Uboweja, E., Hays, M., ... & Grundmann, M. (2019). Mediapipe: A framework for building perception pipelines. arXiv preprint arXiv:1906.08172.\n",
    "<br><br>\n",
    "<h3> Body Tracking</h3>\n",
    "The hand tracking algorith provided below captures the x,y,z keypoints of just the hands, from everyone in frame.  Let's do some tracking and see what we get!\n",
    "<br>\n",
    "First, let's load some packages and set our paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb12dbbb-48c4-4af1-8410-5b60ac443cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"935\" height=\"584\" src=\"https://www.youtube.com/embed/mw8RymohMp0?start=7442\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"935\" height=\"584\" src=\"https://www.youtube.com/embed/mw8RymohMp0?start=7442\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e312957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import cv2\n",
    "import sys\n",
    "import mediapipe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "  \n",
    "#initialize modules\n",
    "drawingModule = mediapipe.solutions.drawing_utils #the module(s) usd from the mediapipe package\n",
    "handsModule = mediapipe.solutions.hands           #the module(s) usd from the mediapipe package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f74790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list all videos in mediafolder\n",
    "mypath = \"./MediaToAnalyze/\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))] # get all files that are in mediatoanalyze\n",
    "#time series output folder\n",
    "foldtime = \"./Timeseries_Output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0bcc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################some preperatory functions and lists for saving the data\n",
    "#take some google classification object and convert it into a string\n",
    "def makegoginto_str(gogobj):\n",
    "    gogobj = str(gogobj).strip(\"[]\")\n",
    "    gogobj = gogobj.split(\"\\n\")\n",
    "    return(gogobj[:-1]) #ignore last element as this has nothing\n",
    "\n",
    "#Hand landmarks\n",
    "markers = ['WRIST', 'THUMB_CMC', 'THUMB_MCP', 'THUMB_IP', 'THUMB_TIP', \n",
    " 'INDEX_MCP', 'INDEX_PIP', 'INDEX_DIP', 'INDEX_TIP', \n",
    " 'MIDDLE_MCP', 'MIDDLE_PIP', 'MIDDLE_DIP','MIDDLE_TIP', \n",
    " 'RING_MCP', 'RING_TIP', 'RING_DIP', 'RING_TIP', \n",
    " 'PINKY_MCP', 'PINKY_PIP', 'PINKY_DIP', 'PINKY_TIP']\n",
    "\n",
    "#make the stringifyd position traces into clean values\n",
    "def listpostions(newsamplemaerks):\n",
    "    tracking_p = []\n",
    "    for value in newsamplelmarks:\n",
    "        stripped = value.split(':', 1)[1]\n",
    "        stripped = stripped.strip() #remove spaces in the string if present\n",
    "        tracking_p.append(stripped) #add to this list  \n",
    "    return(tracking_p)\n",
    "\n",
    "#a function that only retrieves the numerical info in a string\n",
    "def only_numerics(seq):\n",
    "    seq_type= type(seq)\n",
    "    return seq_type().join(filter(seq_type.isdigit, seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cda202",
   "metadata": {},
   "source": [
    "Now we'll pefrorm the actual tracking. This block goes through each video file in your directory, gets the video frames (images) using cv2, creates an output video file, and then collects the tracked points. The saved keypoint coordinates are then drawn onto a copy of the video frame in order to visualize the tracking as well as saved into a .csv file for later analysis. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ecae28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280.0 720.0 50.0\n",
      "1440.0 1080.0 29.97017053449149\n",
      "720.0 480.0 29.97002997002997\n"
     ]
    }
   ],
   "source": [
    "#loop through the frames of the video\n",
    "for ff in onlyfiles:\n",
    "    #capture the video and save some video properties\n",
    "    capture = cv2.VideoCapture(mypath+ff)\n",
    "    frameWidth = capture.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "    frameHeight = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    print(frameWidth, frameHeight, fps ) #print some video info to the console\n",
    "    \n",
    "    #make a video file where we will project keypoints on\n",
    "    samplerate = fps #make the same as current \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID') #(*'XVID')\n",
    "    out = cv2.VideoWriter('Videotracking_output/'+ff[:-4]+'.avi', fourcc, fps= samplerate, frameSize = (int(frameWidth), int(frameHeight))) #make sure that frameheight/width is the same a original\n",
    "\n",
    "    #make a variable list with x, y, z, info where data is appended to\n",
    "    markerxyz = []\n",
    "    for mark in markers:\n",
    "        for pos in ['X', 'Y', 'Z']:\n",
    "            nm = pos + \"_\" + mark\n",
    "            markerxyz.append(nm)\n",
    "    addvariable = ['index', 'confidence', 'hand', 'time']\n",
    "    addvariable.extend(markerxyz)\n",
    "    time = 0\n",
    "    fr = 1\n",
    "    timeseries = [addvariable]\n",
    "    #MAIN ROUTINE\n",
    "         #For finetuning the tracking here check: https://google.github.io/mediapipe/solutions/hands.html\n",
    "    with handsModule.Hands(static_image_mode=False, min_detection_confidence=0.5, min_tracking_confidence=0.75, max_num_hands=6) as hands:\n",
    "         while (True):\n",
    "            ret, frame = capture.read()\n",
    "            if ret == True:\n",
    "                results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                # the results.multi_hand_landmarks should contain sets of x,y,z values for each landmark\n",
    "                # However, they have no label or ID, just raw coordinates. \n",
    "                # we do know which set of coordinates corresponds to which joint:\n",
    "                # see https://google.github.io/mediapipe/solutions/hands.html and figure 2.21 on that page\n",
    "                if results.multi_hand_landmarks != None: \n",
    "                    #attach an id based on location                    \n",
    "                    for handLandmarks, handinfo in zip(results.multi_hand_landmarks,results.multi_handedness):\n",
    "                        # these first few lines just convert the results output into something more workable\n",
    "                        newsamplelmarks = makegoginto_str(handLandmarks.landmark)\n",
    "                        newsamplelmarks = listpostions(newsamplelmarks)\n",
    "                        newsampleinfo = makegoginto_str(handinfo) #get info the hands\n",
    "                        # now we compile the data into a complete row, and add it to our dataframe\n",
    "                        fuldataslice = [fr, newsampleinfo[2], newsampleinfo[3]]\n",
    "                        fuldataslice.extend([str(time)]) #add time\n",
    "                        fuldataslice.extend(newsamplelmarks) #add positions\n",
    "                        timeseries.append(fuldataslice)\n",
    "                        #get information about hand index [0], hand confidence [1], handedness [2]              \n",
    "                        for point in handsModule.HandLandmark:\n",
    "                            normalizedLandmark = handLandmarks.landmark[point]\n",
    "                            # now draw the landmark onto the video frame\n",
    "                            pixelCoordinatesLandmark = drawingModule._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, frameWidth, frameHeight)\n",
    "                            cv2.circle(frame, pixelCoordinatesLandmark, 5, (0, 255, 0), -1)\n",
    "                if results.multi_hand_landmarks == None:\n",
    "                    timeseries.append([\"NA\"]) #add a row of NAs\n",
    "                cv2.imshow('Test hand', frame)\n",
    "                out.write(frame)  #########################################comment this out if you dont wn\n",
    "                time = round(time+1000/samplerate)\n",
    "                fr = fr+1\n",
    "                if cv2.waitKey(1) == 27:\n",
    "                    break\n",
    "            if ret == False:\n",
    "                break\n",
    "    out.release()\n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    ####################################################### data to be written row-wise in csv file\n",
    "    data = timeseries\n",
    "\n",
    "    # opening the csv file in 'w+' mode\n",
    "    file = open(foldtime+ff[:-4]+'.csv', 'w+', newline ='')\n",
    "    #write it\n",
    "    with file:    \n",
    "        write = csv.writer(file)\n",
    "        write.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08974d9",
   "metadata": {},
   "source": [
    "Let's take a first look at the data to see what kind of output we get. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab6d98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./Timeseries_Output/singlefirst_person_sample.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>confidence</th>\n",
       "      <th>hand</th>\n",
       "      <th>time</th>\n",
       "      <th>X_WRIST</th>\n",
       "      <th>Y_WRIST</th>\n",
       "      <th>Z_WRIST</th>\n",
       "      <th>X_THUMB_CMC</th>\n",
       "      <th>Y_THUMB_CMC</th>\n",
       "      <th>Z_THUMB_CMC</th>\n",
       "      <th>...</th>\n",
       "      <th>Z_PINKY_MCP</th>\n",
       "      <th>X_PINKY_PIP</th>\n",
       "      <th>Y_PINKY_PIP</th>\n",
       "      <th>Z_PINKY_PIP</th>\n",
       "      <th>X_PINKY_DIP</th>\n",
       "      <th>Y_PINKY_DIP</th>\n",
       "      <th>Z_PINKY_DIP</th>\n",
       "      <th>X_PINKY_TIP</th>\n",
       "      <th>Y_PINKY_TIP</th>\n",
       "      <th>Z_PINKY_TIP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>score: 0.6093786954879761</td>\n",
       "      <td>label: \"Left\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.635196</td>\n",
       "      <td>0.388808</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.602918</td>\n",
       "      <td>0.389601</td>\n",
       "      <td>-0.018807</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004158</td>\n",
       "      <td>0.634445</td>\n",
       "      <td>0.229552</td>\n",
       "      <td>-0.005760</td>\n",
       "      <td>0.623060</td>\n",
       "      <td>0.205796</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>0.611503</td>\n",
       "      <td>0.190996</td>\n",
       "      <td>0.006322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>score: 0.9999953508377075</td>\n",
       "      <td>label: \"Left\"</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.299859</td>\n",
       "      <td>0.397056</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>0.331408</td>\n",
       "      <td>0.387960</td>\n",
       "      <td>-0.023282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003270</td>\n",
       "      <td>0.282278</td>\n",
       "      <td>0.243715</td>\n",
       "      <td>-0.011688</td>\n",
       "      <td>0.283709</td>\n",
       "      <td>0.214422</td>\n",
       "      <td>-0.018607</td>\n",
       "      <td>0.285403</td>\n",
       "      <td>0.189971</td>\n",
       "      <td>-0.026059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>score: 0.9450348019599915</td>\n",
       "      <td>label: \"Left\"</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.639061</td>\n",
       "      <td>0.392384</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.606290</td>\n",
       "      <td>0.390252</td>\n",
       "      <td>-0.023101</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004803</td>\n",
       "      <td>0.637472</td>\n",
       "      <td>0.231520</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>0.626105</td>\n",
       "      <td>0.206491</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>0.614551</td>\n",
       "      <td>0.191324</td>\n",
       "      <td>0.002124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>score: 0.9999712109565735</td>\n",
       "      <td>label: \"Left\"</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.299803</td>\n",
       "      <td>0.401946</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>0.331562</td>\n",
       "      <td>0.390181</td>\n",
       "      <td>-0.022328</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>0.285484</td>\n",
       "      <td>0.243658</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.288054</td>\n",
       "      <td>0.216183</td>\n",
       "      <td>-0.001764</td>\n",
       "      <td>0.290211</td>\n",
       "      <td>0.194107</td>\n",
       "      <td>-0.006174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>score: 0.9674971699714661</td>\n",
       "      <td>label: \"Left\"</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.641997</td>\n",
       "      <td>0.398204</td>\n",
       "      <td>-0.000030</td>\n",
       "      <td>0.607862</td>\n",
       "      <td>0.392848</td>\n",
       "      <td>-0.025636</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013638</td>\n",
       "      <td>0.642445</td>\n",
       "      <td>0.233575</td>\n",
       "      <td>-0.015278</td>\n",
       "      <td>0.631055</td>\n",
       "      <td>0.207358</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>0.620473</td>\n",
       "      <td>0.190910</td>\n",
       "      <td>-0.009438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                   confidence             hand  time   X_WRIST  \\\n",
       "0    1.0    score: 0.6093786954879761    label: \"Left\"   0.0  0.635196   \n",
       "1    1.0    score: 0.9999953508377075    label: \"Left\"   0.0  0.299859   \n",
       "2    2.0    score: 0.9450348019599915    label: \"Left\"  33.0  0.639061   \n",
       "3    2.0    score: 0.9999712109565735    label: \"Left\"  33.0  0.299803   \n",
       "4    3.0    score: 0.9674971699714661    label: \"Left\"  66.0  0.641997   \n",
       "\n",
       "    Y_WRIST   Z_WRIST  X_THUMB_CMC  Y_THUMB_CMC  Z_THUMB_CMC  ...  \\\n",
       "0  0.388808 -0.000030     0.602918     0.389601    -0.018807  ...   \n",
       "1  0.397056 -0.000041     0.331408     0.387960    -0.023282  ...   \n",
       "2  0.392384 -0.000022     0.606290     0.390252    -0.023101  ...   \n",
       "3  0.401946 -0.000042     0.331562     0.390181    -0.022328  ...   \n",
       "4  0.398204 -0.000030     0.607862     0.392848    -0.025636  ...   \n",
       "\n",
       "   Z_PINKY_MCP  X_PINKY_PIP  Y_PINKY_PIP  Z_PINKY_PIP  X_PINKY_DIP  \\\n",
       "0    -0.004158     0.634445     0.229552    -0.005760     0.623060   \n",
       "1    -0.003270     0.282278     0.243715    -0.011688     0.283709   \n",
       "2    -0.004803     0.637472     0.231520    -0.005495     0.626105   \n",
       "3     0.006947     0.285484     0.243658     0.001423     0.288054   \n",
       "4    -0.013638     0.642445     0.233575    -0.015278     0.631055   \n",
       "\n",
       "   Y_PINKY_DIP  Z_PINKY_DIP  X_PINKY_TIP  Y_PINKY_TIP  Z_PINKY_TIP  \n",
       "0     0.205796    -0.000986     0.611503     0.190996     0.006322  \n",
       "1     0.214422    -0.018607     0.285403     0.189971    -0.026059  \n",
       "2     0.206491    -0.002203     0.614551     0.191324     0.002124  \n",
       "3     0.216183    -0.001764     0.290211     0.194107    -0.006174  \n",
       "4     0.207358    -0.012657     0.620473     0.190910    -0.009438  \n",
       "\n",
       "[5 rows x 67 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(foldtime+ff[:-4]+'.csv')\n",
    "df = pd.read_csv(foldtime+ff[:-4]+'.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f32658",
   "metadata": {},
   "source": [
    "Above we have the first 5 rows of our output data. The first named column, \"index\", provides you with the frame number. Note that each frame may have multiple rows, if multiple hands are tracked in that frame. We also get a label, right or left, and x,y,z coordinates (scaled to 0,1 --- see below) for each keypoint. <br>\n",
    "<h3> Output Details </h3>\n",
    "The 3D coordinate output is certainly an advantage for MediaPipe, as it is able to provide some sense of depth, even if you don't have multiple camera angles or an actual depth image (e.g, as recorded by infrared sensors). The authors of MediaPipe achieve this by training their detector model on a synthetic dataset where they could vary the pose and orientation of the hand in many ways, but always have ground-truth 3D coordinates. As they state in the Zhang et al., 2020 paper:  <i>\"Synthetic dataset: To even better cover the possible hand poses and provide additional supervision for depth, we render a high-quality synthetic hand model over various backgrounds and map it to the corresponding 3D coordinates. We use a commercial 3D hand model that is rigged with 24 bones and includes 36 blendshapes, which control fingers and palm thickness. The model also provides 5 textures with different skin tones. We created video sequences of transformation between hand poses and sampled 100K images from the videos.\" Zhang et al., 2020 </i><br>\n",
    "It is important to note that the depth provided in this output, however, Mediapipe does allow for estimating positions in meters (see https://google.github.io/mediapipe/solutions/hands.html to see which function you need to use for this). In the current example case, a point with x,y coordinates = 0.5, 0.5 would be in the center of the image, while x,y = 0.25, 0.75 would indicate that the point is 1/4 of the way from left to right, and 3/4 of the way from top to bottom (x,y = 0,0 is the top left corner). For depth, it is relative to the wrist. In other words, the wrist is taken as the origin (0 depth), and smaller values are estimated to be closer to the camera, and larger values further away. <br>\n",
    "This relative scaling makes it difficult to compare across videos with different camera set-ups, but is quite intuitive when looking at the coordinates compared to the actual video.\n",
    "\n",
    "<br>However, especially for multi-party data we don't know if the first row in frame 1 is the same hand as the first row in frame 2. Thus, we don't know if a left and right hand belong together as there are multiple persons! We'll cover a potential solution to this in the module on linking and pairing hands. This is easier when there is just one person, as mediapipe does differientate between left and right hand. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
